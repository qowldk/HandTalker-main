{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jNbawRbIFjPW"},"outputs":[],"source":["import numpy as np\n","import os\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n","os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoVrTgUMFoZv"},"outputs":[],"source":["# 코랩\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","data = np.concatenate([\n","    np.load('/content/drive/MyDrive/Colab_Notebooks/total_dataset.npy'),\n","], axis=0)\n","\n","data.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhyzCQQ1FrHK"},"outputs":[],"source":["original_data = data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oXCNa88ZFtYc"},"outputs":[],"source":["x_data = data[:, :, :-1] # 라벨 제거\n","labels = data[:, 0, -1]\n","\n","print(x_data.shape)\n","print(labels.shape) # 라벨의 원핫 인코딩 필요\n","\n","label_length = len(list(set(labels)))\n","print(label_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyuMgNysFvz2"},"outputs":[],"source":["# 원핫 인코딩\n","# ex\n","# 0 [1, 0, 0]\n","# 1 [0, 1, 0]\n","# 2 [0, 0, 1]\n","\n","from tensorflow.keras.utils import to_categorical\n","\n","labels = [int(float(label)) for label in labels]\n","\n","print(labels)\n","y_data = to_categorical(labels, num_classes=label_length)\n","y_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYv022avFxtd"},"outputs":[],"source":["# 데이터를 학습(train)과 검증(validation) 세트로 나눔 -> 모델의 성능을 평가 && 과적합 방지\n","# 과적합: 데이터 크기 작을 때 || 단일 샘플 데이터 세트 장기간 훈련\n","# x: 입력, y: 출력\n","from sklearn.model_selection import train_test_split\n","\n","x_data = x_data.astype(np.float32)\n","y_data = y_data.astype(np.float32)\n","\n","x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=2024) # test_size 조정\n","\n","print(x_train.shape, y_train.shape)\n","print(x_val.shape, y_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GakQcCvTFzpC"},"outputs":[],"source":["# 모델 구조 정의, 컴파일\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import BatchNormalization\n","\n","model = Sequential([\n","    LSTM(256, activation='relu', input_shape=x_train.shape[1:3]), # (512,30,198)[1:3]) : (시퀀스 길이, 특징 수)\n","    BatchNormalization(),\n","\n","    Dropout(0.1),  # Dropout layer 추가 입력, 데이터의 10%를 무작위로 제거, 과적합 방지\n","    Dense(128, activation='relu'), # Dense layer\n","    BatchNormalization(),\n","    Dense(128, activation='relu'), # Dense layer\n","    Dense(label_length, activation='softmax')  # Output layer 유지\n","])\n","\n","optimizer = Adam(learning_rate=0.00005)\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRp7mVrrF1mL"},"outputs":[],"source":["from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from tensorflow.keras.models import load_model\n","\n","history = model.fit(\n","    x_train,\n","    y_train,\n","    validation_data=(x_val, y_val),\n","    epochs=150,\n","    batch_size = 64,  # 배치 크기 설정\n","    callbacks=[\n","        ModelCheckpoint('drive/MyDrive/model_ko.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n","        ReduceLROnPlateau(monitor='val_acc', factor=0.4, patience=10, verbose=1, mode='auto')\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aY3jlLKnF3hl"},"outputs":[],"source":["# 훈련 과정에서 발생한 손실(loss) 및 정확도(accuracy)를 시각화\n","# Adam 값 조정, 모델 계층 조정\n","import matplotlib.pyplot as plt\n","\n","fig, loss_ax = plt.subplots(figsize=(16, 10))\n","acc_ax = loss_ax.twinx()\n","\n","loss_ax.plot(history.history['loss'], 'y', label='train loss') #노랑: 학습 데이터셋 기반 모델의 손실\n","loss_ax.plot(history.history['val_loss'], 'r', label='val loss') #빨강: 검증 데이터셋 기반 모델의 손실\n","loss_ax.set_xlabel('epoch')\n","loss_ax.set_ylabel('loss')\n","loss_ax.legend(loc='upper left')\n","\n","acc_ax.plot(history.history['acc'], 'b', label='train acc') # 파랑: 학습 데이터셋 기반 모델 정확도\n","acc_ax.plot(history.history['val_acc'], 'g', label='val acc') #녹색: 검증 데이터셋 기반 모델 정확도\n","acc_ax.set_ylabel('accuracy')\n","acc_ax.legend(loc='upper left')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iuXaYdDF5Y5"},"outputs":[],"source":["# 모델의 예측 결과와 실제 레이블 간의 다중 레이블 혼동 행렬(multilabel confusion matrix)을 계산하는 작업을 수행\n","\n","from sklearn.metrics import multilabel_confusion_matrix\n","\n","\n","y_pred = model.predict(x_val)\n","\n","multilabel_confusion_matrix(np.argmax(y_val, axis=1), np.argmax(y_pred, axis=1))\n","\n","# 실행 결과 예시\n","# WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","# array([[[94,  1],\n","#         [ 0, 34]],\n","\n","#        [[81,  0],\n","#         [ 0, 47]],\n","\n","#        [[81,  0],\n","#         [ 0, 47]]])\n","\n","# 예시로 해석\n","# True Positive (TP): 모델이 양성(Positive)으로 예측한 것 중에서 실제로 양성인 경우의 수. (94)\n","# True Negative (TN): 모델이 음성(Negative)으로 예측한 것 중에서 실제로 음성인 경우의 수. (34))\n","# False Positive (FP): 모델이 양성(Positive)으로 잘못 예측한 것 중에서 실제로는 음성인 경우의 수. (0: 모델이 양성으로 잘못 예측한 경우가 없음)\n","# False Negative (FN): 모델이 음성(Negative)으로 잘못 예측한 것 중에서 실제로는 양성인 경우의 수. (1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_9pjdCRhgbe"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNhxUW0qk8skPpSXOqAVcuh","gpuType":"T4","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
